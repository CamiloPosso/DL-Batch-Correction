{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "\n",
    "from transformer_batch_corrections import Correction_data\n",
    "from asses_batch_effect import batchless_entropy_estimate, abs_effect_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_TO_DATA   = 'data/peptide_crosstab_1.txt'\n",
    "CrossTab_name = 'synthetic_data_batchPercent_60_groupPercent_10'\n",
    "PATH_TO_DATA   = 'data/' + CrossTab_name + '.txt'\n",
    "CrossTab = pd.read_csv(PATH_TO_DATA,   delimiter = '\\t')\n",
    "minibatch_size = 30\n",
    "megabatch_size = 50\n",
    "batch_size = 20\n",
    "n_batches = 6\n",
    "reg_factor = 5\n",
    "depth = 2\n",
    "run_name = CrossTab_name + \"_dep_{0}_reg_{1}_mb_{2}_rs_{3}\".format(depth, reg_factor, minibatch_size, megabatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'synthetic_data_batchPercent_60_groupPercent_10_dep_2_reg_5_mb_30_rs_50'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Correction_data(CrossTab = CrossTab, depth = depth, reg_factor = reg_factor, \n",
    "                       n_batches = n_batches, batch_size = batch_size, test_size = 2000, \n",
    "                       minibatch_size = minibatch_size, random_state = 107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 report : testing loss is 0.0 while train loss is 26187.257263782896 and absolute effect in testing data is 15.041412855603376\n",
      "\n",
      "Training loss is 3977.3654199792168\n",
      "Training loss is 621.0278531290212\n",
      "Training loss is 17.74242972505898\n",
      "Training loss is 2.924570990567142\n",
      "Training loss is 0.8954279366555175\n",
      "Epoch 5 report : testing loss is 0.4906305566142029 while train loss is 0.8954279366555175 and absolute effect in testing data is 0.6085606086179638\n",
      "\n",
      "Training loss is 0.38517421132275936\n",
      "Training loss is 0.23337556703150852\n",
      "Training loss is 0.18089034014702318\n",
      "Training loss is 0.15364300780573814\n",
      "Training loss is 0.14055458831590148\n",
      "Epoch 10 report : testing loss is 0.315956204289427 while train loss is 0.14055458831590148 and absolute effect in testing data is 0.5674984847677982\n",
      "\n",
      "Training loss is 0.13443984475830556\n",
      "Training loss is 0.12671638236186575\n",
      "Training loss is 0.11856755329602768\n",
      "Training loss is 0.11479546897164634\n",
      "Training loss is 0.12075262533350847\n",
      "Epoch 15 report : testing loss is 0.4654113144567368 while train loss is 0.12075262533350847 and absolute effect in testing data is 0.57112799271182\n",
      "\n",
      "Training loss is 0.12372278473035431\n",
      "Training loss is 0.12441245269161207\n",
      "Training loss is 0.13693585747741843\n",
      "Training loss is 0.10914454625289932\n",
      "Training loss is 0.10961387956222952\n",
      "Epoch 20 report : testing loss is 0.5039382493855917 while train loss is 0.10961387956222952 and absolute effect in testing data is 0.5733935286161305\n",
      "\n",
      "Training loss is 0.11545131062905636\n",
      "Training loss is 0.10699930500126173\n",
      "Training loss is 0.10378473383142316\n",
      "Training loss is 0.10078630920544503\n",
      "Training loss is 0.11816216494893782\n",
      "Epoch 25 report : testing loss is 0.49914456220330466 while train loss is 0.11816216494893782 and absolute effect in testing data is 0.5685251262283723\n",
      "\n",
      "Training loss is 0.11064120782279266\n",
      "Training loss is 0.09996415038651522\n",
      "Training loss is 0.09546720273661591\n",
      "Training loss is 0.10907028290356544\n",
      "Training loss is 0.09163734742390348\n",
      "Epoch 30 report : testing loss is 0.6642836725940298 while train loss is 0.09163734742390348 and absolute effect in testing data is 0.5782611406716924\n",
      "\n",
      "Training loss is 0.08855065293372559\n",
      "Training loss is 0.08862261824886984\n",
      "Training loss is 0.08445553598081547\n",
      "Training loss is 0.08126473797327709\n",
      "Training loss is 0.07726473615315195\n",
      "Epoch 35 report : testing loss is 0.671777994940766 while train loss is 0.07726473615315195 and absolute effect in testing data is 0.5765049885013762\n",
      "\n",
      "Training loss is 0.07109171085239828\n",
      "Training loss is 0.0728299357732927\n",
      "Training loss is 0.07593535634151709\n",
      "Training loss is 0.06918335945258337\n",
      "Training loss is 0.07030734782003213\n",
      "Epoch 40 report : testing loss is 0.7140115556607197 while train loss is 0.07030734782003213 and absolute effect in testing data is 0.5736760742610398\n",
      "\n",
      "Training loss is 0.07362348200634096\n",
      "Training loss is 0.07702224339601065\n",
      "Training loss is 0.07692895719026784\n",
      "Training loss is 0.06813544035210327\n",
      "Training loss is 0.07888038507074463\n",
      "Epoch 45 report : testing loss is 0.6727203064432372 while train loss is 0.07888038507074463 and absolute effect in testing data is 0.5701786289463096\n",
      "\n",
      "Training loss is 0.0647051638503458\n",
      "Training loss is 0.06619003171556437\n",
      "Training loss is 0.06286827771801418\n",
      "Training loss is 0.05812941974544223\n",
      "Training loss is 0.06715280359010826\n",
      "Epoch 50 report : testing loss is 0.702468537619823 while train loss is 0.06715280359010826 and absolute effect in testing data is 0.563934047214475\n",
      "\n",
      "Training loss is 0.06168852810453736\n",
      "Training loss is 0.058368078615970834\n",
      "Training loss is 0.06686042052366019\n",
      "Training loss is 0.07147903948503215\n",
      "Training loss is 0.050506001376274284\n",
      "Epoch 55 report : testing loss is 0.8162939346635149 while train loss is 0.050506001376274284 and absolute effect in testing data is 0.566737015013988\n",
      "\n",
      "Training loss is 0.06784131934672709\n",
      "Training loss is 0.06162149475288034\n",
      "Training loss is 0.05020686588122944\n",
      "Training loss is 0.06160590904850124\n",
      "Training loss is 0.05309547137931033\n",
      "Epoch 60 report : testing loss is 0.7088784289458618 while train loss is 0.05309547137931033 and absolute effect in testing data is 0.5554237075698079\n",
      "\n",
      "Training loss is 0.05557795358049792\n",
      "Training loss is 0.06917178516614035\n",
      "Training loss is 0.0582574723173088\n",
      "Training loss is 0.04921759991508376\n",
      "Training loss is 0.058445827196431314\n",
      "Epoch 65 report : testing loss is 0.604331470046113 while train loss is 0.058445827196431314 and absolute effect in testing data is 0.5462066143512021\n",
      "\n",
      "Training loss is 0.055689673741843145\n",
      "Training loss is 0.06022432819984118\n",
      "Training loss is 0.06484350760243038\n",
      "Training loss is 0.0669593522525334\n",
      "Training loss is 0.06033661937349046\n",
      "Epoch 70 report : testing loss is 0.4009367798230431 while train loss is 0.06033661937349046 and absolute effect in testing data is 0.5294785402439055\n",
      "\n",
      "Training loss is 0.053992854717628505\n",
      "Training loss is 0.056245865484767385\n",
      "Training loss is 0.06006823936459201\n",
      "Training loss is 0.07018720902377672\n",
      "Training loss is 0.051483257331016115\n",
      "Epoch 75 report : testing loss is 0.6462839677141062 while train loss is 0.051483257331016115 and absolute effect in testing data is 0.5436117724972117\n",
      "\n",
      "Training loss is 0.06083419867069297\n",
      "Training loss is 0.05046310410430288\n",
      "Training loss is 0.08087802335413308\n",
      "Training loss is 0.05845744247469907\n",
      "Training loss is 0.06224379392070516\n",
      "Epoch 80 report : testing loss is 0.6606930165433593 while train loss is 0.06224379392070516 and absolute effect in testing data is 0.5396656919149887\n",
      "\n",
      "Training loss is 0.055244404109060924\n",
      "Training loss is 0.050784984962121464\n",
      "Training loss is 0.05871864105343314\n",
      "Training loss is 0.056200111677240226\n",
      "Training loss is 0.06956394898859866\n",
      "Epoch 85 report : testing loss is 0.9565810767498193 while train loss is 0.06956394898859866 and absolute effect in testing data is 0.547889481490655\n",
      "\n",
      "Training loss is 0.05404563943988991\n",
      "Training loss is 0.06891349131969396\n",
      "Training loss is 0.07741258277528927\n",
      "Training loss is 0.05967948413988356\n",
      "Training loss is 0.05021127679427474\n",
      "Epoch 90 report : testing loss is 0.17335931863565945 while train loss is 0.05021127679427474 and absolute effect in testing data is 0.5101283383067214\n",
      "\n",
      "Training loss is 0.05456633332428871\n",
      "Training loss is 0.07323343908624338\n",
      "Training loss is 0.06803030799002298\n",
      "Training loss is 0.06581897203609069\n",
      "Training loss is 0.0557672671822674\n",
      "Epoch 95 report : testing loss is 0.3896062441713386 while train loss is 0.0557672671822674 and absolute effect in testing data is 0.5191044272365971\n",
      "\n",
      "Training loss is 0.07781558312525681\n",
      "Training loss is 0.05812140909530497\n",
      "Training loss is 0.07226392125819874\n",
      "Training loss is 0.07492208815727622\n",
      "Training loss is 0.051009582340076194\n",
      "Epoch 100 report : testing loss is 0.15001439536509809 while train loss is 0.051009582340076194 and absolute effect in testing data is 0.5050506818120121\n",
      "\n",
      "Training loss is 0.059980945839384466\n",
      "Training loss is 0.047449529228792635\n",
      "Training loss is 0.04940092996705049\n",
      "Training loss is 0.07091514279268094\n",
      "Training loss is 0.06315433720844145\n",
      "Epoch 105 report : testing loss is 0.3357312320681906 while train loss is 0.06315433720844145 and absolute effect in testing data is 0.5151047561849386\n",
      "\n",
      "Training loss is 0.06933782792130128\n",
      "Training loss is 0.0707276098623248\n",
      "Training loss is 0.07480393962131737\n",
      "Training loss is 0.05860002881306527\n",
      "Training loss is 0.07639539010669741\n",
      "Epoch 110 report : testing loss is 0.06534563896219892 while train loss is 0.07639539010669741 and absolute effect in testing data is 0.5478139761946105\n",
      "\n",
      "Training loss is 0.07910856421662774\n",
      "Training loss is 0.0750713301149266\n",
      "Training loss is 0.06587169574617782\n",
      "Training loss is 0.07813098163556947\n",
      "Training loss is 0.05720795283428065\n",
      "Epoch 115 report : testing loss is 0.11775182661518012 while train loss is 0.05720795283428065 and absolute effect in testing data is 0.531364567652387\n",
      "\n",
      "Training loss is 0.07364579497588113\n",
      "Training loss is 0.056039738919494854\n",
      "Training loss is 0.0670927635143845\n",
      "Training loss is 0.09593836172123699\n",
      "Training loss is 0.08025641999083033\n",
      "Epoch 120 report : testing loss is 0.06326064433416034 while train loss is 0.08025641999083033 and absolute effect in testing data is 0.540702528973458\n",
      "\n",
      "Training loss is 0.07299450213992717\n",
      "Training loss is 0.07136701247187247\n",
      "Training loss is 0.08158713583907301\n",
      "Training loss is 0.07790439154403413\n",
      "Training loss is 0.06241557600938228\n",
      "Epoch 125 report : testing loss is 0.1169203633572337 while train loss is 0.06241557600938228 and absolute effect in testing data is 0.512562165255992\n",
      "\n",
      "Training loss is 0.08312239440247805\n",
      "Training loss is 0.06429252323601783\n",
      "Training loss is 0.07084045637643446\n",
      "Training loss is 0.09017851593727444\n",
      "Training loss is 0.07098054808762912\n",
      "Epoch 130 report : testing loss is 0.07942435470254339 while train loss is 0.07098054808762912 and absolute effect in testing data is 0.5128576280195466\n",
      "\n",
      "Training loss is 0.057153780138770316\n",
      "Training loss is 0.07216800835305986\n",
      "Training loss is 0.05687522339043567\n",
      "Training loss is 0.06117381116048579\n",
      "Training loss is 0.062057367559958616\n",
      "Epoch 135 report : testing loss is 0.06939284961552065 while train loss is 0.062057367559958616 and absolute effect in testing data is 0.5215694606507715\n",
      "\n",
      "Training loss is 0.08186522014272847\n",
      "Training loss is 0.06843934561780875\n",
      "Training loss is 0.062228816092375794\n",
      "Training loss is 0.0700045555498134\n",
      "Training loss is 0.08488478908523713\n",
      "Epoch 140 report : testing loss is 0.2566472695265773 while train loss is 0.08488478908523713 and absolute effect in testing data is 0.5158854679883726\n",
      "\n",
      "Training loss is 0.07094056531928743\n",
      "Training loss is 0.07483445989136622\n",
      "Training loss is 0.10460771985251911\n",
      "Training loss is 0.07249691445346575\n",
      "Training loss is 0.07046124905990474\n",
      "Epoch 145 report : testing loss is 0.15329595827024683 while train loss is 0.07046124905990474 and absolute effect in testing data is 0.5163981429985327\n",
      "\n",
      "Training loss is 0.07890648355540948\n",
      "Training loss is 0.05284108112282606\n",
      "Training loss is 0.05861439228084525\n",
      "Training loss is 0.07017153889835077\n",
      "Training loss is 0.061630057051401135\n",
      "Epoch 150 report : testing loss is 0.1287676911400589 while train loss is 0.061630057051401135 and absolute effect in testing data is 0.5091966985354084\n",
      "\n",
      "Training loss is 0.0798521440399853\n",
      "Training loss is 0.07707366741641662\n",
      "Training loss is 0.08367976747273541\n",
      "Training loss is 0.06860346902971401\n",
      "Training loss is 0.07367835765726105\n",
      "Epoch 155 report : testing loss is 0.12573251658825305 while train loss is 0.07367835765726105 and absolute effect in testing data is 0.5158631705007486\n",
      "\n",
      "Training loss is 0.08151226217905096\n",
      "Training loss is 0.06712427423085826\n",
      "Training loss is 0.061205318196348474\n",
      "Training loss is 0.06996108708354425\n",
      "Training loss is 0.06951210642689142\n",
      "Epoch 160 report : testing loss is 0.09342784653917445 while train loss is 0.06951210642689142 and absolute effect in testing data is 0.5254639121369806\n",
      "\n",
      "Training loss is 0.07754606332682423\n",
      "Training loss is 0.0649041115957208\n",
      "Training loss is 0.057220838071551136\n",
      "Training loss is 0.0593843358509961\n",
      "Training loss is 0.057271862107936826\n",
      "Epoch 165 report : testing loss is 0.08435447695620035 while train loss is 0.057271862107936826 and absolute effect in testing data is 0.5128720144461894\n",
      "\n",
      "Training loss is 0.07869051509687869\n",
      "Training loss is 0.06795243874621136\n",
      "Training loss is 0.062202940212165984\n",
      "Training loss is 0.07596527635747059\n",
      "Training loss is 0.07210245286315181\n",
      "Epoch 170 report : testing loss is 0.2522065054326694 while train loss is 0.07210245286315181 and absolute effect in testing data is 0.5137384127991793\n",
      "\n",
      "Training loss is 0.07709978229777438\n",
      "Training loss is 0.07300491885094948\n",
      "Training loss is 0.06386599839217486\n",
      "Training loss is 0.08428696013177861\n",
      "Training loss is 0.07234594205582257\n",
      "Epoch 175 report : testing loss is 0.07132983987736596 while train loss is 0.07234594205582257 and absolute effect in testing data is 0.5158797890912004\n",
      "\n",
      "Training loss is 0.05618575414366179\n",
      "Training loss is 0.07612167619814439\n",
      "Training loss is 0.06811364953568767\n",
      "Training loss is 0.07857912805716773\n",
      "Training loss is 0.06812197342132931\n",
      "Epoch 180 report : testing loss is 0.1300084786059278 while train loss is 0.06812197342132931 and absolute effect in testing data is 0.5075560196690541\n",
      "\n",
      "Training loss is 0.09006615478895849\n",
      "Training loss is 0.09723250047324385\n",
      "Training loss is 0.05675991958024588\n",
      "Training loss is 0.05799115540751794\n",
      "Training loss is 0.08282641643904161\n",
      "Epoch 185 report : testing loss is 0.1504057703075054 while train loss is 0.08282641643904161 and absolute effect in testing data is 0.5261723271778583\n",
      "\n",
      "Training loss is 0.06562611718614256\n",
      "Training loss is 0.06908381509202045\n",
      "Training loss is 0.062056261601921424\n",
      "Training loss is 0.07114883475137371\n",
      "Training loss is 0.07115570875999502\n",
      "Epoch 190 report : testing loss is 0.07266049635202397 while train loss is 0.07115570875999502 and absolute effect in testing data is 0.513574266365193\n",
      "\n",
      "Training loss is 0.06943751559448047\n",
      "Training loss is 0.05361782962364822\n",
      "Training loss is 0.07379733653240672\n",
      "Training loss is 0.10166198020774436\n",
      "Training loss is 0.07123477220478289\n",
      "Epoch 195 report : testing loss is 0.14509089933971053 while train loss is 0.07123477220478289 and absolute effect in testing data is 0.5203401582682035\n",
      "\n",
      "Training loss is 0.07111866298952758\n",
      "Training loss is 0.06429348889917029\n",
      "Training loss is 0.06451342586671864\n",
      "Training loss is 0.05951501417041278\n",
      "Training loss is 0.06676844988677183\n",
      "Epoch 200 report : testing loss is 0.28142234377943726 while train loss is 0.06676844988677183 and absolute effect in testing data is 0.5242729361345649\n",
      "\n",
      "Training loss is 0.06940447549358411\n",
      "Training loss is 0.08175195693870616\n",
      "Training loss is 0.06859991433632955\n",
      "Training loss is 0.05794100826281563\n",
      "Training loss is 0.06495371817844994\n",
      "Epoch 205 report : testing loss is 0.24375845814584235 while train loss is 0.06495371817844994 and absolute effect in testing data is 0.5454194023505006\n",
      "\n",
      "Training loss is 0.0749345185175978\n",
      "Training loss is 0.06708933540432854\n",
      "Training loss is 0.08888402390957653\n",
      "Training loss is 0.067876025064065\n",
      "Training loss is 0.07367934812886792\n",
      "Epoch 210 report : testing loss is 0.2990668977973727 while train loss is 0.07367934812886792 and absolute effect in testing data is 0.5456170798910526\n",
      "\n",
      "Training loss is 0.06402132948123186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test\u001b[39m.\u001b[39;49mtrain_model(epochs \u001b[39m=\u001b[39;49m \u001b[39m1001\u001b[39;49m, report_frequency \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, abs_effect_cutoff \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, resample_training \u001b[39m=\u001b[39;49m megabatch_size, run_name \u001b[39m=\u001b[39;49m run_name)\n",
      "File \u001b[1;32mc:\\Users\\Camilo\\Documents\\GitHub\\DL-Batch-Correction\\entropy_correction\\transformer_batch_corrections.py:292\u001b[0m, in \u001b[0;36mCorrection_data.train_model\u001b[1;34m(self, epochs, abs_effect_cutoff, resample_training, minibatch_bias, report_frequency, run_name)\u001b[0m\n\u001b[0;32m    290\u001b[0m raw_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective(y, z)\n\u001b[0;32m    291\u001b[0m loss \u001b[39m=\u001b[39m raw_loss \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_objective(z)\n\u001b[1;32m--> 292\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    294\u001b[0m training_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(raw_loss)\n",
      "File \u001b[1;32mc:\\Users\\Camilo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Camilo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test.train_model(epochs = 1001, report_frequency = 5, abs_effect_cutoff = 0, resample_training = megabatch_size, run_name = run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Correction_data(CrossTab = CrossTab, depth = 1, reg_factor = 1, \n",
    "                       n_batches = n_batches, batch_size = batch_size, test_size = 2000, \n",
    "                       minibatch_size = 50, random_state = 107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 report : testing loss is 7.794723518458238 while full loss is 8.037390866981749 and absolute effect in testing data is 14.741614062569662\n",
      "\n",
      "Training loss is 8025.952670275298\n",
      "Training loss is 5176.697261517657\n",
      "Training loss is 2033.530536685791\n",
      "Training loss is 477.64863815783315\n",
      "Training loss is 108.51638850844076\n",
      "Epoch 5 report : testing loss is 0.11275257189992302 while full loss is 0.1087386845503715 and absolute effect in testing data is 1.4363193286892293\n",
      "\n",
      "Training loss is 33.96146534195165\n",
      "Training loss is 13.72008416405465\n",
      "Training loss is 6.635500116888256\n",
      "Training loss is 3.4713905806383853\n",
      "Training loss is 1.953576565481834\n",
      "Epoch 10 report : testing loss is 0.016558505565042212 while full loss is 0.012351479558861305 and absolute effect in testing data is 0.6839096737996287\n",
      "\n",
      "Training loss is 1.1165445461267582\n",
      "Training loss is 0.6939324630739534\n",
      "Training loss is 0.4606007501866254\n",
      "Training loss is 0.3245346425804666\n",
      "Training loss is 0.26234710550893475\n",
      "Epoch 15 report : testing loss is 0.01192002122222615 while full loss is 0.007848579471425473 and absolute effect in testing data is 0.6071646389408927\n",
      "\n",
      "Training loss is 0.22914994782657888\n",
      "Training loss is 0.22245527156290332\n",
      "Training loss is 0.21281379246491972\n",
      "Training loss is 0.1785122398485479\n",
      "Training loss is 0.175174604406001\n",
      "Epoch 20 report : testing loss is 0.011309559159774676 while full loss is 0.007265335615203635 and absolute effect in testing data is 0.591449780822583\n",
      "\n",
      "Training loss is 0.17305439716282262\n"
     ]
    }
   ],
   "source": [
    "test.train_model(epochs = 21, report_frequency = 5, abs_effect_cutoff = 0, resample_training = 60, run_name = \"synth_reg_1_mb_50_rs_60_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Correction_data(CrossTab = CrossTab, depth = 1, reg_factor = 0.3, \n",
    "                       n_batches = 6, batch_size = 10, test_size = 7000, \n",
    "                       minibatch_size = 350, random_state = 107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.train_model(epochs = 301, report_frequency = 5, abs_effect_cutoff = 0.5, robust_cutoff = 0, minibatch_bias = 0.65, run_name = \"robust_metric_bias_shuffle_0.65_minbatch_350\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Correction_data(CrossTab = CrossTab, depth = 1, reg_factor = 0.3, \n",
    "                       n_batches = 6, batch_size = 10, test_size = 7000, \n",
    "                       minibatch_size = 100, random_state = 107)\n",
    "test.train_model(epochs = 301, report_frequency = 5, abs_effect_cutoff = 0.5, robust_cutoff = 0, minibatch_bias = 0.65, run_name = \"robust_metric_bias_shuffle_0.65_minbatch_100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0060f6c6241d242352d70eb9953069bcbd774cf8ab494c384436a30f34fd96d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
